---
output: github_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(readxl)
library(naniar)
```

# Data pipeline

-   *Existing*

First, I need to make sure all the files are of L1 Arabic speakers.

```{r}
# read in the key which includes speakers' ids and L1s
id <- read_excel("Private/user_id-anon_id-key_EDITED.xlsm")

# only select the relevant columns
id <- id %>% 
  select(user_id, native_language)

# the lowest id value in TalkBank is 848 and the highest is 1351. This filters out participants who are not involved in the TalkBank data and only leaves the L1 Arabic.
Arbids <- id %>% 
  filter(user_id > 847, user_id < 1352, native_language == "Arabic")

# extract the ids as a pattern 
onlyarb <- str_extract(Arbids$user_id, "\\d{3,}")

# this is the file that includes the TalkBank transcripts
speech <- "data/Vercellotti"

# this reads all the file paths to an object from which I can filter non-Arabic L1s
speech <- list.files(speech, pattern = "*.cha", full.names = TRUE)

#  collapse the different strings (ids) into one string separated by | for matching
onlyarb <- str_flatten(onlyarb, "|")
 
# filter out non-Arabic L1s
speech <- str_subset(speech, onlyarb)
```




I need to create a variable for participants' ids, and a variable for their level.

```{r}
#this gives the ids listed in the file names
ids <- str_extract(speech, "\\d{3,}")

# this gives the level listed in the file names
levels <- str_extract(speech, "_\\d{1}[A-Z]")
levels <- str_remove_all(levels, "_|[A-Z]") 

# changing the names in level to something more meaningful
levels <- str_replace_all(levels, c("3" = "low-int", "4" = "high-int", "5" = "low-adv"))
```





Before I create the data frame, I need to remove the first few lines of metadata in the transcripts

```{r}
# read the lines 
lines <- map(speech, read_lines)

# put the maximum number of lines in an object
max <- map_int(lines, length) %>% max()

# this is mapping the rep function over every element in the list. The number of NAs added to each line is calculated by the max number of line - the number of lines of each element.
evenlines <- map(lines, ~c(.x, rep(NA, max - length(.x))))

# now that all the elements have even number of lines, I can put it in a tibble to use slice to remove lines I don't want. 
tibblelines <- as_tibble(evenlines, .name_repair = "unique")

# this removes the first few lines. For some reason slice_tail selects from the start not the end, and slice_head selects from the end.
removedlines <- slice_tail(tibblelines, n = -9)

# save the sliced transcripts as a list in an object to put them in a tibble with ids and levels later.
removedlines <- as.list(removedlines) 

```



creating the data frame

```{r}
# create the data frame/tibble with ids and sliced transcripts
speechdf <- tibble(ID = ids, Level = levels, Transcript = removedlines)

speechdf
speechdf %>% 
  count(Level)

speechdf %>% 
  filter(Level == "low-int") %>% 
  count(ID, Level)
```





Cleaning the texts from `.cha` formatting.

```{r}
speechdf$Transcript <- speechdf$Transcript %>% 
  str_remove_all("\\bNA\\b|\\d+_\\d+|\\*\\d+|\\d{3}|@...|\\\\t|\\\n|c\\(|[^\\w|\\s]|\\buh\\s|\\bah\\s|\\bPAR|\\bPAR0") %>% 
  str_replace_all("_", " ")

head(speechdf$Transcript)

```





I am going to create 3 data frames to analyze the article use for each level

```{r}
lowint <- speechdf %>% 
  filter(Level == "low-int")

highint <- speechdf %>% 
  filter(Level == "high-int")

lowadv <- speechdf %>% 
  filter(Level == "low-adv")

```





I will POS tag the data frames to make analyzing the contexts of article use possible

```{r}
library(udpipe)

# downloading the model only needs to be done once.
udpipe_download_model(language = "english-ewt")

#loading the language model
model <- udpipe_load_model(file = ("english-ewt-ud-2.5-191206.udpipe"))
```



<!-- instead of creating three different df create a column with the tagged dfs -->
<!-- ```{r} -->
<!-- ANNOTspeech <- speechdf %>% -->
<!--   mutate(annoTXT = map(Transcript, ~ udpipe_annotate(model, .x) %>% as_tibble)) -->

<!-- low_int <-  low_int %>% mutate(annotTXT = paste(low_int$token, "/", low_int$xpos, collapse = " ", sep = ""))  -->


<!-- # saved the file so I don't have to run udpipe_annotate() every time -->
<!-- saveRDS(ANNOTspeech, "articles-analysis.csv") -->


<!-- ANNOTspeech <- readRDS("articles-analysis.csv") -->

<!-- head(ANNOTspeech$annoTXT, 1) -->

<!-- ``` -->




POS tagging the data frames.

```{r}
# low_int <- udpipe_annotate(model, x = lowint$Transcript) %>% 
#   as_tibble()
```

```{r}
# high_int <- udpipe_annotate(model, x = highint$Transcript) %>% 
#   as_tibble()
```

```{r}
# low_adv <-  udpipe_annotate(model, x = lowadv$Transcript) %>% 
#   as_tibble()
```



a list of common nouns:
```{r}
commonNN <- c("admiration/NNC", "advice/NNC", "air/NNC", "anger/NNC", "anticipation/NNC", 
              "assistance/NNC", "awareness/NNC", "bacon/NNC", "baggage/NNC", "blood/NNC", 
              "bravery/NNC", "chess/NNC", "clay/NNC", "clothing/NNC", "coal/NNC", "compliance/NNC", 
              "comprehension/NNC", "confusion/NNC", "consciousness/NNC", "cream/NNC", "darkness/NNC", 
              "diligence/NNC", "dust/NNC", "education/NNC", "empathy/NNC", "enthusiasm/NNC", "envy/NNC", 
              "equality/NNC", "equipment/NNC", "evidence/NNC", "feedback/NNC", "fitness/NNC", "flattery/NNC", 
              "foliage/NNC", "fun/NNC", "furniture/NNC", "garbage/NNC", "gold/NNC", "gossip/NNC", 
              "grammar/NNC", "gratitude/NNC", "gravel/NNC", "guilt/NNC", "happiness/NNC", "hardware/NNC", 
              "hate/NNC", "hay/NNC", "health/NNC", "heat/NNC", "help/NNC", "hesitation/NNC", "homework/NNC", 
              "honesty/NNC", "honor/NNC", "hospitality/NNC", "hostility/NNC", "humanity/NNC", "humility/NNC", 
              "ice/NNC", "immortality/NNC", "independence/NNC", "information/NNC", "integrity/NNC", 
              "intimidation/NNC", "jargon/NNC", "jealousy/NNC", "jewelry/NNC", "justice/NNC", "knowledge/NNC", 
              "literacy/NNC", "logic/NNC", "luck/NNC", "lumber/NNC", "luggage/NNC", "mail/NNC", "management/NNC", 
              "merchandise/NNC", "milk/NNC", "morale/NNC", "mud/NNC", "music/NNC", "nonsense/NNC", "oppression/NNC", 
              "optimism/NNC", "oxygen/NNC", "participation/NNC", "pay/NNC", "peace/NNC", "perseverance/NNC", 
              "pessimism/NNC", "pneumonia/NNC", "poetry/NNC", "police/NNC", "pride/NNC", "privacy/NNC", 
              "propaganda/NNC", "public/NNC", "punctuation/NNC", "recovery/NNC", "rice/NNC", "rust/NNC", 
              "satisfaction/NNC", "shame/NNC", "sheep/NNC", "slang/NNC", "software/NNC", "spaghetti/NNC", 
              "stamina/NNC", "starvation/NNC", "steam/NNC", "steel/NNC", "stuff/NNC", "support/NNC", "sweat/NNC", 
              "thunder/NNC", "timber/NNC", "toil/NNC", "traffic/NNC", "training/NNC", "trash/NNC", 
              "understanding/NNC", "valor/NNC", "vehemence/NNC", "violence/NNC", "warmth/NNC", "waste/NNC", 
              "weather/NNC", "wheat/NNC", "wisdom/NNC", "work/NNC", "food/NNC", "ham/NNC", "beef/NNC", "lettuce/NNC", "tv/NNC")

```


Now, I will annotate the texts with POS tags so I can analyze the contexts in addition to tagging mass nouns, which the tagger does not mark.

```{r}
#I'll annotate the texts with pos
# low_int <-  low_int %>% mutate(annotTXT = paste(low_int$token, "/", low_int$xpos, collapse = " ", sep = "")) 

# this pattern was created to replace /NN with /NNC for mass nouns  
patternn <- paste0("\\b(", paste(sub("/NNC", "", commonNN), collapse = "|"), ")\\b/NN")

# Replace /NN with /NNC to tag mass nouns
# low_int$annotTXT <- str_replace_all(low_int$annotTXT, patternn, "\\1/NNC")


```

```{r}
# high_int <-  high_int %>% mutate(annotTXT = paste(high_int$token, "/", high_int$xpos, collapse = " ", sep = ""))
# Replace /NN with /NNC to tag mass nouns
# high_int$annotTXT <- str_replace_all(high_int$annotTXT, patternn, "\\1/NNC")

```

```{r}
 # low_adv <-  low_adv %>% mutate(annotTXT = paste(low_adv$token, "/", low_adv$xpos, collapse = " ", sep = ""))

 # low_adv$annotTXT <- str_replace_all(low_adv$annotTXT, patternn, "\\1/NNC")
```


I have saved each tagged df so that I don't have to run the tagging operations every time. Below these chunks, I have set the code for loading. Feel free to use that to skip running the tagger.
```{r}
# saveRDS(low_int, "data/lowint.csv")
# saveRDS(high_int, "data/highint.csv")
# saveRDS(low_adv, "data/lowadv.csv")
```


instead of tagging every time, i made these tagged dfs.
```{r}
low_int <- readRDS("Private/lowint.csv")
high_int <- readRDS("Private/highint.csv")
low_adv <- readRDS("Private/lowadv.csv")
```


Here, I will attempt to extract the contexts of article use for low-intermediate, high-intermediate, and low-advanced.

Low-intermediate
```{r}
LI_correct <- low_int$annotTXT %>% 
  str_unique() %>% 
  str_remove_all("\\b../NN") %>% 
  str_remove_all("a/DT\\screating/NN\\s|a/DT\\smam/NN\\s|a/DT\\sfewer/JJR\\sgarage/NN\\s|a/DT\\sthe/DT\\stopic/NN\\s|a/DT\\sfewer/JJR\\sparking/NN\\s") %>% 
  str_extract_all("(a|an)/DT\\s(\\w+/\\w+\\s)?\\w+/NN\\s") %>% 
  unlist()
str_unique(LI_correct)


# Instance of incorrect use with plural nouns
LI_incorrectPL <- low_int$annotTXT %>% 
  str_unique() %>% 
  str_remove_all("\\b../NN") %>% 
  str_remove_all("a/DT\\sfew/JJ\\sbuses/NNS") %>% 
  str_extract_all("(a|an)/DT\\s(\\w+/\\w+\\s)?\\w+/NNS") %>% 
  unlist()
str_unique(LI_incorrectPL)

# instances of incorrect use with mass nouns
LI_incorrectMS <- low_int$annotTXT %>% 
  str_unique() %>% 
  str_remove_all("\\b../NN") %>% 
  str_extract_all("(a|an)/DT\\s(\\w+/\\w+\\s)?\\w+/NNC") %>% 
  unlist()
str_unique(LI_incorrectMS)


# I created a pattern to exclude so that I can focus on cases where indefinite articles were not used. 

pattern <- "\\w+/(DT|CD)\\s(\\w+/\\w+\\s)?(\\w+|someone|something|somewhere|everything|everyone)/NN\\s"
patt <- "\\w+/VB.\\s(\\w+/\\w+\\s)?(someone|something|somewhere|everything|everyone|anyone|anybody|anything|nothing|pollution|experience|oil|bread|pasta|hair|football|soccer|worhis|cheese|english|forget|shopping|Weam|childhood)/NN\\s"

# pulls out instances where a/an should have been used.
LI_null <- low_int$annotTXT %>%
  str_unique() %>% 
  str_remove_all("\\b../NN\\s") %>% 
  str_remove_all("\\b./NN\\s") %>% 
  str_split(pattern) %>%
  str_split(patt) %>% 
  str_remove_all("um/NN") %>%
  str_remove_all(nofit) %>% 
  str_extract_all("\\w+/\\w+\\s\\w+/VB.\\s(\\w+/\\w+\\s)?\\w+/NN\\s") %>% 
  unlist()
str_unique(LI_null)  
  str_subset("are/VBP")
  
# These are all the contexts that don't fit the requirement  
nofit <-  str_c("worhe/PRP has/VBZ requality/NN ", "he/PRP took/VBD care/NN ",  "always/RB playing/VBG barbie/NN ", "I/PRP woke/VBD up/RP morning/NN ", "he/PRP is/VBZ best/JJS friend/NN ", "and/CC hes/VBZ stole/NN ", "he/PRP had/VBD many/JJ problem/NN ",  "there/EX are/VBP different/JJ way/NN ", "cheese/NN has/VBZ different/JJ kind/NN ",  "they/PRP want/VBP creat/NN ", "he/PRP had/VBD he/PRP assist/NN ", "I/PRP woke/VBD up/RP saturday/NN ",  "in/IN was/VBD tra/NN ",   "we/PRP had/VBD dinner/NN ", "he/PRP lives/VBZ in/IN sultana/NN ", "you/PRP are/VBP you/PRP name/NN ",   "guys/NNS was/VBD best/JJS friend/NN ", "I/PRP miss/VBP him/PRP morcoord/NN ", "I/PRP was/VBD walk/NN ", "mother/NN\\stook/VBD\\scare/NN", "there/EX\\sare/VBP\\sfewer/JJR\\sfewer/NN", "he/PRP\\sdid/VBD\\smany/JJ\\sthing/NN", "he/PRP\\sread/VBD\\smore/JJR\\sstory/NN", "we/PRP\\swe/VBD\\sin/IN\\sfact/NN", "friend/NN\\sis/VBZ\\sname/NN", "I/PRP\\swant/VBP\\stalk/NN", sep = "|") %>% 
  str_replace_all(" ", "\\\\s")
no

```




High-intermediate
```{r}
HI_correct <- high_int$annotTXT %>% 
  str_unique() %>% 
  str_remove_all("\\b../NN\\s") %>% 
  str_remove_all("\\b./NN\\s") %>% 
  str_remove_all("pollution|a/DT\\smany/JJ\\sfuture/NN|a/DT\\smissing/NN|a/DT\\sskin/NN\\scancer/NN|a/DT\\slil/NN|a/DT\\sArab/JJ\\sGulf/NN|a/DT\\sgood/JJ\\sethics/NN|a/DT\\stha/NN|an/DT\\sd/JJ\\smoreover/NN|a/DT\\swalking/NN|a/DT\\smoney/NN") %>% 
  str_remove_all("\\bPAR/NN|\\bPAR0/NN|\\bPAR0") %>% 
  str_extract_all("(a|an)/DT\\s(\\w+/\\w+\\s)?\\w+/NN\\s") %>% 
  unlist()
  
str_unique(HI_correct)

# instances of incorrect use of plural nouns
HI_incorrectPL<- high_int$annotTXT %>% 
  str_unique() %>% 
  str_remove_all("\\b../NN\\s") %>% 
  str_remove_all("\\b./NN\\s") %>% 
  str_remove_all("a/DT\\scouple/NN\\syears/NNS") %>% 
  str_extract_all("(a|an)/DT\\s(\\w+/\\w+\\s)?\\w+/NNS") %>% 
  unlist()
HI_incorrectPL

# instances of incorrect use of mass nouns
HI_incorrectMS <- high_int$annotTXT %>% 
  str_unique() %>% 
  str_remove_all("\\b../NN\\s") %>% 
  str_remove_all("\\b./NN\\s") %>%
  str_remove_all("a/DT\\swhole/JJ\\ssheep/NNC") %>%
  str_extract_all("(a|an)/DT\\s(\\w+/\\w+\\s)?\\w+/NNC") %>% 
  unlist()
str_unique(HI_incorrectMS)


pattern <- "\\w+/(DT|CD)\\s(\\w+/\\w+\\s)?\\w+/NN\\s"
patt <- "\\w+/VB.\\s(\\w+/\\w+\\s)?(someone|something|somewhere|everything|everyone|anyone|anybody|anything|nothing|pollution|experience|oil|bread|pasta|hair|football|soccer|worhis|cheese|english|forget|shopping|Weam|childhood)/NN\\s"

HI_null <- high_int$annotTXT %>%
  str_unique() %>% 
  str_split(pattern) %>%
  str_split(patt) %>% 
  str_split("are/VBP|something/NN\\s") %>% 
  str_remove_all("\\b../NN\\s") %>% 
  str_remove_all("\\b./NN\\s") %>% 
  str_remove_all("\\bPAR/NN|\\bPAR0/NN|\\bPAR0") 
  str_extract_all("\\w+/VB.\\s(\\w+/\\w+\\s){0,2}\\w+/NN\\s") %>% 
  unlist()
str_unique(HI_null)
  str_subset("many/")
head(HI_null, 12)
```

Low-advanced
```{r}
LA_correct <- low_adv$annotTXT %>% 
  str_unique() %>% 
  str_remove_all("\\b../NN\\s") %>% 
  str_remove_all("\\b./NN\\s") %>% 
  str_remove_all("a/DT\\senter/NN|a/DT\\skra/NN|a/DT\\smoney/NN|a/DT\\scensorshi/NN\\scen/NN") %>% 
  str_extract_all("(a|an)/DT\\s(\\w+/\\w+\\s)?\\w+/NN\\s") %>% 
  unlist()
str_unique(LA_correct)

# instances of incorrect use with plural nouns
LA_incorrectPL <- low_adv$annotTXT %>% 
  str_unique() %>% 
  str_remove_all("\\b../NN\\s") %>% 
  str_remove_all("\\b./NN\\s") %>% 
  str_remove_all("a/DT\\sfew/JJ\\sscenes/NNS|a/DT\\sfew/JJ\\smovies/NNS") %>%
  str_extract_all("(a|an)/DT\\s(\\w+/\\w+\\s)?\\w+/NNS") %>% 
  unlist()
LA_incorrectPL

# instances of incorrect use with mass nouns
LA_incorrectMS <- low_adv$annotTXT %>% 
  str_unique() %>% 
  str_remove_all("\\b../NN\\s") %>% 
  str_remove_all("\\b./NN\\s") %>%
  str_extract_all("(a|an)/DT\\s(\\w+/\\w+\\s)?\\w+/NNC") %>% 
  unlist()
LA_incorrectMS


LA_null <- low_adv$annotTXT %>% 
  str_unique() %>% 
  str_split(pattern) %>%
  str_split(patt) %>% 
  str_split("are/VBP") %>% 
  str_remove_all("\\b../NN\\s") %>% 
  str_remove_all("\\b./NN\\s") %>% 
  str_extract_all("\\w+/VB.\\s(\\w+/\\w+\\s){0,2}\\w+/NN\\s") %>% 
  unlist()
str_unique(LA_null) 
  str_subset("are/VBP")
```



Here, I put the extracted contexts into a df and added a columns tagging level and use.

low-intermediate
```{r}
lowint_corr <- str_unique(LI_correct) %>% 
  as_tibble() %>% 
rename("LI_correct" = "value") %>% 
  mutate(status = "low-int_correct", .after = LI_correct)
lowint_corr

# incoorect with plurals
lowint_incorrPL <- str_unique(LI_incorrectPL) %>%
  as_tibble() %>% 
rename("LI_incorrectPL" = "value") %>% 
  mutate(status = "low-int_incorrect-PL", .after = LI_incorrectPL)

# incorrect with mass
lowint_incorrMS <- str_unique(LI_incorrectMS) %>%
  as_tibble() %>% 
rename("LI_incorrectMS" = "value") %>% 
  mutate(status = "low-int_incorrect-MS", .after = LI_incorrectMS)



lowint_null <- str_unique(LI_null) %>% 
  as_tibble() %>% 
rename("LI_null" = "value") %>% 
  mutate(status = "low-int_null", .after = LI_null)
lowint_null
```



high-intermediate
```{r}
highint_corr <- str_unique(HI_correct) %>% 
  as_tibble() %>% 
rename("HI_correct" = "value") %>% 
  mutate(status = "high-int_correct", .after = HI_correct) 
highint_corr 

#incorrect with plurals
highint_incorrPL <- str_unique(HI_incorrectPL) %>%
  as_tibble() %>% 
rename("HI_incorrectPL" = "value") %>% 
  mutate(status = "high-int_incorrect-PL", .after = HI_incorrectPL)

# incorrect with mass
highint_incorrMS <- str_unique(HI_incorrectMS) %>%
  as_tibble() %>% 
rename("HI_incorrectMS" = "value") %>% 
  mutate(status = "high-int_incorrect-MS", .after = HI_incorrectMS)

highint_null <- str_unique(HI_null) %>% 
  as_tibble() %>% 
rename("HI_null" = "value") %>% 
  mutate(status = "high-int_null", .after = HI_null)

```


low-advanced
```{r}
lowad_corr <- str_unique(LA_correct) %>%
  as_tibble() %>% 
rename("LA_correct" = "value") %>% 
  mutate(status = "low-adv_correct", .after = LA_correct)
```


```{r}
# incorrect with plurals
lowad_incorrPL <- str_unique(LA_incorrectPL) %>%
  as_tibble() %>% 
rename("LA_incorrectPL" = "value") %>% 
  mutate(status = "low-adv_incorrectPL", .after = LA_incorrectPL)

# incorrect with mass
lowad_incorrMS <- str_unique(LA_incorrectMS) %>%
  as_tibble() %>% 
rename("LA_incorrectMS" = "value") %>% 
  mutate(status = "low-adv_incorrectMS", .after = LA_incorrectMS)
```


```{r}
lowad_null <- str_unique(LA_null) %>% 
  as_tibble() %>% 
rename("LA_null" = "value") %>% 
  mutate(status = "low-adv_null", .after = LA_null)
```



Creating the variables from the tibbles created above to get the "final" form of the data from which the analysis can be done.
```{r}
Status <- c(lowint_corr$status, lowint_incorrPL$status, lowint_incorrMS$status, lowint_null$status, highint_corr$status, highint_incorrPL$status, highint_incorrMS$status, highint_null$status, lowad_corr$status, lowad_incorrPL$status, lowad_incorrMS$status, lowad_null$status)

Article_contexts <- c(lowint_corr$LI_correct, lowint_incorrPL$LI_incorrectPL, lowint_incorrMS$LI_incorrectMS, lowint_null$LI_null, highint_corr$HI_correct, highint_incorrPL$HI_incorrectPL, highint_incorrMS$HI_incorrectMS, highint_null$HI_null, lowad_corr$LA_correct, lowad_incorrPL$LA_incorrectPL, lowad_incorrMS$LA_incorrectMS,  lowad_null$LA_null)

articles <- tibble(Article_contexts, Status)

# separating level and correctness of use into two columns
articles <- articles %>% 
  separate(Status, into = c("Level", "Use"), sep = "_")

articles
#This is the final form of the data. For any new changes I make I'll run this again to overwrite the old version
saveRDS(articles, "data/articles_final.csv")
articles <- readRDS("data/articles_final.csv")

```


For this project the main goal is to see how many times the participants used articles, correctly or incorrectly, vs omitted them from obligatory contexts. To this end, a count of instances will be used. Some more work needs to be done to ensure all the examples included in each category is appropriate, but the analysis goal will remain the same for now.

```{r}
articles %>% 
  count(Level, Use)
articles
  ggplot(aes(as.factor(Use)))+
  geom_bar()

articles %>% 
```
maybe consider adding participant ids, and don't forget to split incorrect into PL and MS

