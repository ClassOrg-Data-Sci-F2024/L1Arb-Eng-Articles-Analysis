---
output: github_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(readxl)
```

# Data pipeline

-   *Existing*

First, I need to make sure all the files are of L1 Arabic speakers.

```{r}
# read in the key which includes speakers' ids and L1s
id <- read_excel("Private/user_id-anon_id-key_EDITED.xlsm")

# only select the relevant columns
id <- id %>% 
  select(user_id, native_language)

# the lowest id value in TalkBank is 848 and the highest is 1351. This filters out participants who are not involved in the TalkBank data and only leaves the L1 Arabic.
Arbids <- id %>% 
  filter(user_id > 847, user_id < 1352, native_language == "Arabic")

# extract the ids as a pattern 
onlyarb <- str_extract(Arbids$user_id, "\\d{3,}")

# this is the file that includes the TalkBank transcripts
speech <- "data/Vercellotti"

# this reads all the file paths to an object from which I can filter non-Arabic L1s
speech <- list.files(speech, pattern = "*.cha", full.names = TRUE)

#  collapse the different strings (ids) into one string separated by | for matching
onlyarb <- str_flatten(onlyarb, "|")
 
# filter out non-Arabic L1s
speech <- str_subset(speech, onlyarb)
```




I need to create a variable for participants' ids, and a variable for their level.

```{r}
#this gives the ids listed in the file names
ids <- str_extract(speech, "\\d{3,}")

# this gives the level listed in the file names
levels <- str_extract(speech, "_\\d{1}[A-Z]")
levels <- str_remove_all(levels, "_|[A-Z]") 

# changing the names in level to something more meaningful
levels <- str_replace_all(levels, c("3" = "low-int", "4" = "high-int", "5" = "low-adv"))
```





Before I create the data frame, I need to remove the first few lines of metadata in the transcripts

```{r}
# read the lines 
lines <- map(speech, read_lines)

# put the maximum number of lines in an object
max <- map_int(lines, length) %>% max()

# this is mapping the rep function over every element in the list. The number of NAs added to each line is calculated by the max number of line - the number of lines of each element.
evenlines <- map(lines, ~c(.x, rep(NA, max - length(.x))))

# now that all the elements have even number of lines, I can put it in a tibble to use slice to remove lines I don't want. 
tibblelines <- as_tibble(evenlines, .name_repair = "unique")

# this removes the first few lines. For some reason slice_tail selects from the start not the end, and slice_head selects from the end.
removedlines <- slice_tail(tibblelines, n = -9)

# save the sliced transcripts as a list in an object to put them in a tibble with ids and levels later.
removedlines <- as.list(removedlines) 

```





creating the data frame

```{r}
# create the data frame/tibble with ids and sliced transcripts
speechdf <- tibble(ID = ids, Level = levels, Transcript = removedlines)
```





Cleaning the texts from `.cha` formatting.

```{r}
speechdf$Transcript <- speechdf$Transcript %>% 
  str_remove_all("\\bNA\\b|\\d+_\\d+|\\*\\d+|\\d{3}|@...|\\\\t|\\\n|c\\(|[^\\w|\\s]|\\buh\\s|\\bah\\s") %>%
  str_replace_all("_", " ")

```





I am going to create 3 data frames to analyze the article use for each level

```{r}
lowint <- speechdf %>% 
  filter(Level == "low-int")

highint <- speechdf %>% 
  filter(Level == "high-int")

lowadv <- speechdf %>% 
  filter(Level == "low-adv")

```





I will POS tag the data frames to make analyzing the contexts of article use possible

```{r}
library(udpipe)

# downloading the model only needs to be done once.
udpipe_download_model(language = "english-ewt")

#loading the language model
model <- udpipe_load_model(file = ("english-ewt-ud-2.5-191206.udpipe"))
```





POS tagging the data frames

```{r}
low_int <- udpipe_annotate(model, x = lowint$Transcript) %>% 
  as_tibble() 

  
high_int <- udpipe_annotate(model, x = highint$Transcript) %>% 
  as_tibble() 


low_adv <-  udpipe_annotate(model, x = lowadv$Transcript) %>% 
  as_tibble()
```





Now, I will annotate the texts with POS tags so I can analyze the contexts

```{r}
#I'll annotate the texts with pos
low_int <-  low_int %>% mutate(annotTXT = paste(low_int$token, "/", low_int$xpos, collapse = " ", sep = ""))

high_int <-  high_int %>% mutate(annotTXT = paste(high_int$token, "/", high_int$xpos, collapse = " ", sep = ""))

low_adv <-  low_adv %>% mutate(annotTXT = paste(low_adv$token, "/", low_adv$xpos, collapse = " ", sep = ""))


```





Here, I will attempt to extract the contexts of article use.

Low-intermediate
```{r}
LI_correct <- low_int$annotTXT %>% 
  str_extract_all("(a|an)/DT\\s(\\w+/\\w+\\s)?\\w+/NN\\s") %>% 
  unlist() %>% 
  str_unique()

# I still need to find a list for common mass nouns because it is incorrect to a/an with mass nouns also, but the tagger does not tag them
LI_incorrect <- low_int$annotTXT %>% 
  str_extract_all("(a|an)/DT\\s(\\w+/\\w+\\s)?\\w+/NNS") %>% 
  unlist() %>% 
  str_unique()


# I have not started thinking about how to get indefinite article omissions
LI_null <- low_int$annotTXT %>%
```


High-intermediate
```{r}
HI_correct <- high_int$annotTXT %>% 
  str_extract_all("(a|an)/DT\\s(\\w+/\\w+\\s)?\\w+/NN\\s") %>% 
  unlist() %>% 
  str_unique()

# I still need to find a list for common mass nouns because it is incorrect to a/an with mass nouns also, but the tagger does not tag them
HI_incorrect<- high_int$annotTXT %>% 
  str_extract_all("(a|an)/DT\\s(\\w+/\\w+\\s)?\\w+/NNS") %>% 
  unlist() %>% 
  str_unique()


# I have not started thinking about how to get indefinite article omissions
HI_null <- high_int$annotTXT %>%
```


Low-advanced
```{r}
LA_correct <- low_adv$annotTXT %>% 
  str_extract_all("(a|an)/DT\\s(\\w+/\\w+\\s)?\\w+/NN\\s") %>% 
  unlist() %>% 
  str_unique()

# I still need to find a list for common mass nouns because it is incorrect to a/an with mass nouns also, but the tagger does not tag them
LA_incorrect <- low_adv$annotTXT %>% 
  str_extract_all("(a|an)/DT\\s(\\w+/\\w+\\s)?\\w+/NNS") %>% 
  unlist() %>% 
  str_unique()

# I have not started thinking about how to get indefinite article omissions
LA_null <- low_adv$annotTXT %>% 
```

Once I have the matches I need, I will create a df for each level in which the three uses of indefinite articles will have a column.
